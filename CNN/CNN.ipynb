{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining non-linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    shape = x.shape\n",
    "    x = x.reshape(-1)\n",
    "    null = np.argwhere(x < 0)\n",
    "    x[null] = 0\n",
    "    x = x.reshape(shape)\n",
    "    return x\n",
    "\n",
    "def softmax(x):\n",
    "    d = np.exp(x).sum()\n",
    "    return np.exp(x)/d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(inputs, kernels, kernel_size, stride, padding, activation, kernel_weights):\n",
    "    [n_r,n_c,d] = inputs.shape\n",
    "    #number of filters\n",
    "    F = kernels\n",
    "    [f_r,f_c] = kernel_size\n",
    "    bias = np.random.normal(0,1,kernels)\n",
    "    if padding == \"same\":\n",
    "        out = np.zeros(((int)(n_r/stride),(int)(n_c/stride),F))\n",
    "        #p_r == # of layers to be padded.\n",
    "        [p_r,p_c] = ((int)((stride*(n_r/stride-1) + f_r - n_r)/2),(int)((stride*(n_c/stride-1) + f_c - n_c)/2))\n",
    "        inp_padded = np.zeros((n_r+2*p_r,n_c+2*p_c,d))\n",
    "        #r == row step, c == column step\n",
    "        [r,c] = (np.arange(0,n_r+2*p_r-f_r+1,stride),np.arange(0,n_c+2*p_c-f_c+1,stride))\n",
    "        #transforming input into its padded version\n",
    "        for i in range (p_r,n_r+p_r):\n",
    "            for j in range (p_c,n_c+p_c):\n",
    "                for k in range (0,d):\n",
    "                    inp_padded[i][j][k] = inputs[i-p_r][j-p_c][k]\n",
    "    if padding == \"valid\":\n",
    "        out = np.zeros(((int)((n_r - f_r)/stride) + 1, (int)((n_c - f_c)/stride) + 1,F), dtype=float)\n",
    "        [r,c] = (np.arange(0,n_r-f_r+1,stride),np.arange(0,n_c-f_r+1,stride))\n",
    "    #chunck from input volume\n",
    "    A = np.zeros((f_r,f_c,d))\n",
    "    sub_ker = np.zeros((f_r,f_c,d))\n",
    "    #performing convolution             \n",
    "    for i in r:\n",
    "        for j in c:\n",
    "            for k in range(0,d):\n",
    "                for k_r in range(i,i+f_r):\n",
    "                    for k_c in range(j, j+f_c):\n",
    "                        if padding == \"same\":\n",
    "                            A[k_r-i][k_c-j][k] = inp_padded[k_r][k_c][k]\n",
    "                        if padding == \"valid\":\n",
    "                            A[k_r-i][k_c-j][k] = inputs[k_r][k_c][k]\n",
    "            for f in range(0,F):\n",
    "                for l in range(0,d):\n",
    "                    for m in range(0,f_r):\n",
    "                        for n in range(0,f_c):\n",
    "                            sub_ker[m][n][l] = kernel_weights[f][m][n][l]\n",
    "                out[np.asscalar(np.argwhere(r == i))][np.asscalar(np.argwhere(c == j))][f] = (np.multiply(sub_ker,A)).sum() + np.asscalar(bias[f])  \n",
    "    out_unactivated = out\n",
    "    if activation == \"sigmoid\":\n",
    "        out = sigmoid(out)\n",
    "    if activation == \"tanh\":\n",
    "        out = np.tanh(out)\n",
    "    if activation == \"ReLU\":\n",
    "        out = ReLU(out)\n",
    "    return out,out_unactivated,inp_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pooling Layer of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_function(c_out, stride, pool_type):\n",
    "    [n_r,n_c,d] = c_out.shape\n",
    "    p_out = np.zeros(((int)(n_r/stride), (int)(n_c/stride),d), dtype=float)\n",
    "    #r = row step, c = column step\n",
    "    [r,c] = (np.arange(0,n_r-stride+1,stride),np.arange(0,n_c-stride+1,stride))\n",
    "    A = np.zeros((stride,stride))\n",
    "    #input type matrix with ones at pooled outputs and zeros at non-pooled items\n",
    "    pooled_values = np.zeros(c_out.shape)\n",
    "    for i in r:\n",
    "        for j in c:\n",
    "            for k in range(0,d):\n",
    "                for k_r in range(i,i+stride):\n",
    "                    for k_c in range(j, j+stride):\n",
    "                        A[k_r-i][k_c-j] = c_out[k_r][k_c][k]\n",
    "                if pool_type == \"max\":\n",
    "                    pooled_values[np.argwhere(A == np.max(A))[0][0]+i][np.argwhere(A == np.max(A))[0][1]+j][k] = 1\n",
    "                    p_out[np.asscalar(np.argwhere(r == i))][np.asscalar(np.argwhere(c == j))][k] =  np.max(A)\n",
    "                if pool_type == \"average\":\n",
    "                    p_out[np.asscalar(np.argwhere(r == i))][np.asscalar(np.argwhere(c == j))][k] =  np.mean(A.reshape(-1))\n",
    "                if pool_type == \"l2norm\":\n",
    "                    p_out[np.asscalar(np.argwhere(r == i))][np.asscalar(np.argwhere(c == j))][k] =  np.linalg.norm(A)\n",
    "    return p_out,pooled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "X_train = np.vstack([img.reshape((28, 28)) for img in mnist.train.images])\n",
    "y_train = mnist.train.labels\n",
    "X_test = np.vstack([img.reshape(28, 28) for img in mnist.test.images])\n",
    "y_test = mnist.test.labels\n",
    "del mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1,28,28,1))\n",
    "X_test = X_test.reshape((-1,28,28,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = 2\n",
    "kernels = [8,16]\n",
    "kernel_sizes = [(5,5),(5,5)]\n",
    "pool_type = \"max\"\n",
    "pool_size = [(2,2),(2,2)]\n",
    "pool_strides = [2,2]\n",
    "cnn_activations = [\"ReLU\",\"ReLU\"]\n",
    "dense_units = 1024\n",
    "logit_units = 10\n",
    "r = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    a = np.argwhere(x > 0)\n",
    "    b = np.argwhere(x <= 0)\n",
    "    for i in range(0,a.shape[0]):\n",
    "        x[a[i][0]][a[i][1]][a[i][2]] = 1\n",
    "    for i in range(0,b.shape[0]):\n",
    "        x[b[i][0]][b[i][1]][b[i][2]] = 1\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve2D(inputs,filters):\n",
    "    out = np.zeros((filters.shape[2],inputs.shape[0]-filters.shape[0]+1,inputs.shape[1]-filters.shape[1]+1,inputs.shape[2]))\n",
    "    A = np.zeros((filters.shape[0],filters.shape[1]))\n",
    "    sub_ker = np.zeros(A.shape)\n",
    "    for i in range(0,filters.shape[2]):\n",
    "        for r_i in range(0,filters.shape[0]):\n",
    "            for c_i in range(0,filters.shape[1]):\n",
    "                sub_ker[r_i][c_i] = filters[r_i][c_i][i]\n",
    "        for j in range(0,inputs.shape[2]):\n",
    "            for k_i in range(0,out.shape[1]):\n",
    "                    for k_j in range(0,out.shape[2]):\n",
    "                        for k_r in range(k_i,k_i+filters.shape[0]):\n",
    "                            for k_c in range(k_j,k_j+filters.shape[1]):\n",
    "                                A[k_r-k_i][k_c-k_j] = inputs[k_r][k_c][j]\n",
    "                        out[i][k_i][k_j][j] = np.multiply(sub_ker,A).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve3D(inputs,filters):\n",
    "    out = np.zeros((inputs.shape[0],inputs.shape[1],filters.shape[3]))\n",
    "    input_padded = np.zeros((inputs.shape[0]+filters.shape[1]-1,inputs.shape[1]+filters.shape[2]-1,inputs.shape[2]))\n",
    "    [p_r,p_c] = [(int)((filters.shape[1]-1)/2),(int)((filters.shape[2]-1)/2)]\n",
    "    for k in range(0,inputs.shape[2]):\n",
    "        for i in range(p_r,input_padded.shape[0]-p_r):\n",
    "            for j in range(p_c,input_padded.shape[1]-p_c):\n",
    "                input_padded[i][j][k] = inputs[i-p_r][j-p_c][k]\n",
    "    A = np.zeros((filters.shape[1],filters.shape[2],filters.shape[0]))\n",
    "    sub_ker = np.zeros(A.shape)\n",
    "    for k in range(0,filters.shape[3]):\n",
    "        for i in range(0,filters.shape[1]):\n",
    "            for j in range(0,filters.shape[2]):\n",
    "                for d in range(0,filters.shape[0]):\n",
    "                    sub_ker[i][j][d] = filters[d][i][j][k]\n",
    "        for i in range(0,out.shape[0]):\n",
    "            for j in range(0,out.shape[1]):\n",
    "                for r_i in range(i,i+filters.shape[1]):\n",
    "                    for r_j in range(j,j+filters.shape[2]):\n",
    "                        for d in range(0,filters.shape[0]):\n",
    "                            A[r_i-i][r_j-j][d] = input_padded[r_i][r_j][d]\n",
    "                out[i][j][k] = np.multiply(A,sub_ker).sum()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolve(inputs,filters,kernel_size):\n",
    "    inputs = inputs.reshape((28,28))\n",
    "    input_padded = np.zeros((28+kernel_size[0]-1,28+kernel_size[1]-1))\n",
    "    sub_ker = np.zeros((28,28))\n",
    "    A = np.zeros((28,28))\n",
    "    out = np.zeros((filters.shape[2],kernel_size[0],kernel_size[1]))\n",
    "    for i in range((int)((kernel_size[0]-1)/2),input_padded.shape[0]-(int)((kernel_size[0]-1)/2)):\n",
    "            for j in range((int)((kernel_size[1]-1)/2),input_padded.shape[1]-(int)((kernel_size[1]-1)/2)):\n",
    "                input_padded[i][j] = inputs[i-(int)((kernel_size[0]-1)/2)][j-(int)((kernel_size[0]-1)/2)]\n",
    "    for k in range(0,filters.shape[2]):\n",
    "        for i in range(0,28):\n",
    "            for j in range(0,28):\n",
    "                sub_ker[i][j] = filters[i][j][k]\n",
    "        for i in range(0,kernel_size[0]):\n",
    "            for j in range(0,kernel_size[1]):\n",
    "                for r_i in range(i,i+28):\n",
    "                    for r_j in range(j,j+28):\n",
    "                        A[r_i-i][r_j-j] = input_padded[r_i][r_j]\n",
    "                out[k][i][j] = np.multiply(A,sub_ker).sum()\n",
    "    return out.reshape((out.shape[0],out.shape[1],out.shape[2],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#weight initialization\n",
    "w_conv2D1 = np.random.normal(0,1,(kernels[0],kernel_sizes[0][0],kernel_sizes[0][1],1))\n",
    "w_conv2D1 = (1/np.sqrt(np.size(w_conv2D1)/kernels[0]))*w_conv2D1\n",
    "w_conv2D2 = np.random.normal(0,1,(kernels[1],kernel_sizes[1][0],kernel_sizes[1][1],kernels[0]))\n",
    "w_conv2D2 = (1/np.sqrt(np.size(w_conv2D2)/kernels[1]))*w_conv2D2\n",
    "w_dense = np.random.normal(0,1,(7*7*kernels[1],dense_units))\n",
    "w_dense = (1/np.sqrt(7*7*kernels[1]))*w_dense\n",
    "w_logit = np.random.normal(0,1,(dense_units,logit_units))\n",
    "w_logit = (1/np.sqrt(dense_units))*w_logit\n",
    "\n",
    "for epochs in range(0,15):\n",
    "    batch_size = 55\n",
    "    \n",
    "    #shuffling the train_images\n",
    "    shuffle = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    c = 0\n",
    "    \n",
    "    for i in shuffle:\n",
    "        X_train[c],X_train[i] = X_train[i],X_train[c]\n",
    "        y_train[c],y_train[i] = y_train[i],y_train[c]\n",
    "        c += 1\n",
    "    batches = X_train.reshape((batch_size,(int)(X_train.shape[0]/batch_size),X_train.shape[1],X_train.shape[2],X_train.shape[3]))\n",
    "    \n",
    "    #takes sum of gradients in a batch\n",
    "    w_logit_g_sum = np.zeros((dense_units, logit_units))\n",
    "    w_dense_g_sum = np.zeros((7*7*kernels[1], dense_units))\n",
    "    w_conv2D2_g_sum = np.zeros((kernels[1], kernel_sizes[1][0], kernel_sizes[1][1], kernels[0]))\n",
    "    w_conv2D1_g_sum = np.zeros((kernels[0], kernel_sizes[0][0], kernel_sizes[0][1], 1))\n",
    "    \n",
    "    for i in range(0,(int)(X_train.shape[0]/batch_size)):\n",
    "        for j in range(0,batch_size):\n",
    "            image = batches[i][j]\n",
    "            \n",
    "            #forward pass\n",
    "            \n",
    "            #convolution layer - 1\n",
    "            [out_conv2D1,out_conv2D1_unactivated,inp_conv2D1_padded] = conv_layer(image, kernels[0], kernel_sizes[0], 1, \"same\", cnn_activations[0],w_conv2D1)\n",
    "            \n",
    "            #pool layer - 1 \n",
    "            [pool_out1,pooled_values1] = pooling_function(out_conv2D1, pool_strides[0], pool_type)\n",
    "            \n",
    "            #convolution layer - 2\n",
    "            [out_conv2D2,out_conv2D2_unactivated,inp_conv2D2_padded] = conv_layer(pool_out1, kernels[1], kernel_sizes[1], 1, \"same\", cnn_activations[1],w_conv2D2)\n",
    "            \n",
    "            #pool_layer - 2\n",
    "            [pool_out2,pooled_values2] = pooling_function(out_conv2D2, pool_strides[1], pool_type)\n",
    "            \n",
    "            #flattening pool_layer output\n",
    "            flatten = pool_out2.reshape(-1)\n",
    "            \n",
    "            #dense layer nodes\n",
    "            dense_out = np.dot(flatten,w_dense)\n",
    "            \n",
    "            #out\n",
    "            logit_out = softmax(np.dot(dense_out,w_logit))\n",
    "            \n",
    "            error = y_train[i*batch_size+j] - logit_out\n",
    "            \n",
    "            #finding gradients with a backward pass\n",
    "            \n",
    "            #gradients of weights to logit layer from dense layer\n",
    "            w_logit_g = np.asmatrix(dense_out).T*np.asmatrix(error)\n",
    "            w_logit_g_sum += w_logit_g\n",
    "            \n",
    "            #gradients of weights to dense layer from flatten layer\n",
    "            w_dense_g = np.asmatrix(flatten).T*(np.asmatrix(error)*w_logit_g.T)\n",
    "            w_dense_g_sum += w_dense_g\n",
    "            \n",
    "            w_mlp_g = w_dense_g*(np.asmatrix(error)*w_logit_g.T).T\n",
    "            w_mlp_g = np.asarray(w_mlp_g)\n",
    "            \n",
    "            #transforming gradients behind pool layer - 2\n",
    "            w_g_afterpool2 = w_mlp_g.reshape((7,7,kernels[1]))\n",
    "            w_g_pool2 = np.multiply(pooled_values2,np.repeat(np.repeat(w_g_afterpool2,2,axis=1),2,axis=0))\n",
    "            w_g_filters2 = np.multiply(w_g_pool2,relu_derivative(out_conv2D2_unactivated))\n",
    "            \n",
    "            #gradients of weights in convolution layer - 2\n",
    "            w_conv2D2_g = convolve2D(np.asarray(inp_conv2D2_padded),np.asarray(w_g_filters2))\n",
    "            w_conv2D2_g_sum += w_conv2D2_g\n",
    "            \n",
    "            #transforming gradients behind pool layer - 1\n",
    "            w_g_afterpool1 = convolve3D(np.asarray(w_g_filters2),np.asarray(w_conv2D2_g))\n",
    "            w_g_pool1 = np.multiply(pooled_values1,np.repeat(np.repeat(w_g_afterpool1,2,axis=1),2,axis=0))\n",
    "            w_g_filters1 = np.multiply(w_g_pool1,relu_derivative(out_conv2D1_unactivated))\n",
    "            \n",
    "            #gradients of weights in convolution layer - 1\n",
    "            w_conv2D1_g = convolve(image,w_g_filters1,kernel_sizes[0])\n",
    "            w_conv2D1_g_sum += w_conv2D1_g\n",
    "        #updating weights with batch gradients    \n",
    "        w_conv2D1 -= r*(1/batch_size)*w_conv2D1_g_sum\n",
    "        w_conv2D2 -= r*(1/batch_size)*w_conv2D2_g_sum\n",
    "        w_dense -= r*(1/batch_size)*w_dense_g_sum\n",
    "        w_logit -= r*(1/batch_size)*w_logit_g_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_conv2D1 = np.random.normal(0,1,(kernels[0],kernel_sizes[0][0],kernel_sizes[0][1],1))\n",
    "w_conv2D1 = (1/np.sqrt(np.size(w_conv2D1)/kernels[0]))*w_conv2D1\n",
    "w_conv2D2 = np.random.normal(0,1,(kernels[1],kernel_sizes[1][0],kernel_sizes[1][1],kernels[0]))\n",
    "w_conv2D2 = (1/np.sqrt(np.size(w_conv2D2)/kernels[1]))*w_conv2D2\n",
    "w_dense = np.random.normal(0,1,(7*7*kernels[1],dense_units))\n",
    "w_dense = (1/np.sqrt(7*7*kernels[1]))*w_dense\n",
    "w_logit = np.random.normal(0,1,(dense_units,logit_units))\n",
    "w_logit = (1/np.sqrt(dense_units))*w_logit\n",
    "\n",
    "for epochs in range(0,15):\n",
    "    \n",
    "    shuffle = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(shuffle)\n",
    "    train_error = np.zeros(15)\n",
    "    \n",
    "    for i in shuffle:\n",
    "        image = X_train[i]\n",
    "        \n",
    "        #forward pass\n",
    "        [out_conv2D1,out_conv2D1_unactivated,inp_conv2D1_padded] = conv_layer(image, kernels[0], kernel_sizes[0], 1, \"same\", cnn_activations[0],w_conv2D1)\n",
    "        \n",
    "        [pool_out1,pooled_values1] = pooling_function(out_conv2D1, pool_strides[0], pool_type)\n",
    "        \n",
    "        [out_conv2D2,out_conv2D2_unactivated,inp_conv2D2_padded] = conv_layer(pool_out1, kernels[1], kernel_sizes[1], 1, \"same\", cnn_activations[1],w_conv2D2)\n",
    "        \n",
    "        [pool_out2,pooled_values2] = pooling_function(out_conv2D2, pool_strides[1], pool_type)\n",
    "        \n",
    "        flatten = pool_out2.reshape(-1)\n",
    "        \n",
    "        dense_out = np.dot(flatten,w_dense)\n",
    "        \n",
    "        logit_out = softmax(np.dot(dense_out,w_logit))\n",
    "        \n",
    "        error = y_train[i] - logit_out\n",
    "        \n",
    "        #finding gradients with a backward pass\n",
    "        w_logit_g = np.asmatrix(dense_out).T*np.asmatrix(error)\n",
    "        \n",
    "        w_dense_g = np.asmatrix(flatten).T*(np.asmatrix(error)*w_logit_g.T)\n",
    "        \n",
    "        w_mlp_g = w_dense_g*(np.asmatrix(error)*w_logit_g.T).T\n",
    "        w_mlp_g = np.asarray(w_mlp_g)\n",
    "        \n",
    "        w_g_afterpool2 = w_mlp_g.reshape((7,7,kernels[1]))\n",
    "        w_g_pool2 = np.multiply(pooled_values2,np.repeat(np.repeat(w_g_afterpool2,2,axis=1),2,axis=0))\n",
    "        w_g_filters2 = np.multiply(w_g_pool2,relu_derivative(out_conv2D2_unactivated))\n",
    "        \n",
    "        w_conv2D2_g = convolve2D(np.asarray(inp_conv2D2_padded),np.asarray(w_g_filters2))\n",
    "        \n",
    "        w_g_afterpool1 = convolve3D(np.asarray(w_g_filters2),np.asarray(w_conv2D2_g))\n",
    "        w_g_pool1 = np.multiply(pooled_values1,np.repeat(np.repeat(w_g_afterpool1,2,axis=1),2,axis=0))\n",
    "        w_g_filters1 = np.multiply(w_g_pool1,relu_derivative(out_conv2D1_unactivated))\n",
    "        \n",
    "        w_conv2D1_g = convolve(image,w_g_filters1,kernel_sizes[0])\n",
    "        \n",
    "        w_conv2D1 -= r*w_conv2D1_g\n",
    "        w_conv2D2 -= r*w_conv2D2_g\n",
    "        w_dense -= r*w_dense_g \n",
    "        w_logit -= r*w_logit_g\n",
    "        \n",
    "        train_error[epochs] += (-1*(np.multiply(y_train[i],np.log(logit_out)).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
